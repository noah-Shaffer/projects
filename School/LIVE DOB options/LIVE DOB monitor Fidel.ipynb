{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab62967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñ±Ô∏è  Mouse position (1/sec) - Ctrl+C to quit\n",
      "X: 1147 Y:  326\n",
      "‚úÖ Stopped.\n"
     ]
    }
   ],
   "source": [
    "# cmd shift 5 for area, this code for x_y\n",
    "import pyautogui\n",
    "import time\n",
    "\n",
    "print(\"üñ±Ô∏è  Mouse position (1/sec) - Ctrl+C to quit\")\n",
    "try:\n",
    "    while True:\n",
    "        x, y = pyautogui.position()\n",
    "        print(f\"\\rX: {x:4d} Y: {y:4d}\", end='', flush=True)\n",
    "        time.sleep(1)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n‚úÖ Stopped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77c443d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded as API: https://deepdoctection-deepdoctection.hf.space ‚úî\n",
      "üöÄ Starting order book monitor (every 2.0s)\n",
      "Table region: (50, 227, 283, 250)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f_/1bh6mfq97bxdwzwnr49jf8nm0000gn/T/ipykernel_50643/745242070.py:53: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(str(table))[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1] 2025-12-06 09:48:15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f_/1bh6mfq97bxdwzwnr49jf8nm0000gn/T/ipykernel_50643/745242070.py:53: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(str(table))[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2] 2025-12-06 09:48:25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f_/1bh6mfq97bxdwzwnr49jf8nm0000gn/T/ipykernel_50643/745242070.py:53: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(str(table))[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  3] 2025-12-06 09:48:34\n",
      "\n",
      " Stopped by user ‚Äî starting CSV cleanup...\n",
      "‚úÖ Cleaned CSV saved ‚Üí fidelity_orderbook_clean.csv\n",
      "First 20 columns:\n",
      "['timestamp', 'exch_b_1', 'size_b_1', 'bid_1', 'ask_1', 'size_a_1', 'exch_a_1', 'exch_b_2', 'size_b_2', 'bid_2', 'ask_2', 'size_a_2', 'exch_a_2', 'exch_b_3', 'size_b_3', 'bid_3', 'ask_3', 'size_a_3', 'exch_a_3', 'exch_b_4']\n",
      "\n",
      "Sample preview:\n",
      "             timestamp exch_b_1  size_b_1   bid_1   ask_1  size_a_1 exch_a_1\n",
      "0  2025-12-06 09:48:15     ARCX         1  454.61  454.64       160     XNMS\n",
      "1  2025-12-06 09:48:25     ARCX         1  454.61  454.64       160     XNMS\n"
     ]
    }
   ],
   "source": [
    "import pyautogui\n",
    "from PIL import Image, ImageEnhance, ImageFilter, ImageOps\n",
    "from gradio_client import Client, handle_file\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# CONFIGURATION\n",
    "TABLE_X, TABLE_Y, TABLE_WIDTH, TABLE_HEIGHT = 50, 227, 283, 250\n",
    "INTERVAL_SECONDS = 2.0\n",
    "CSV_FILE = \"fidelity_orderbook_log.csv\"\n",
    "CLEAN_CSV_FILE = \"fidelity_orderbook_clean.csv\"\n",
    "\n",
    "# REUSE CLIENT (critical for speed)\n",
    "client = Client(\"deepdoctection/deepdoctection\")\n",
    "\n",
    "print(f\"üöÄ Starting order book monitor (every {INTERVAL_SECONDS}s)\")\n",
    "print(f\"Table region: ({TABLE_X}, {TABLE_Y}, {TABLE_WIDTH}, {TABLE_HEIGHT})\")\n",
    "\n",
    "iteration = 0\n",
    "try:\n",
    "    while True:\n",
    "        start_time = time.time()\n",
    "        iteration += 1\n",
    "\n",
    "        try:\n",
    "            # 1. CAPTURE SCREENSHOT\n",
    "            screenshot = pyautogui.screenshot(region=(TABLE_X, TABLE_Y, TABLE_WIDTH, TABLE_HEIGHT))\n",
    "\n",
    "            # 2. PREPROCESS IMAGE\n",
    "            img = screenshot.convert(\"RGB\")\n",
    "            gray = ImageEnhance.Contrast(img.convert(\"L\")).enhance(1.2)\n",
    "            gray = gray.filter(ImageFilter.SHARPEN)\n",
    "            inverted = ImageOps.invert(gray)\n",
    "            binary = inverted.point(lambda p: 0 if p > 140 else 255)\n",
    "            binary.save(\"processed_table_region.png\")\n",
    "\n",
    "            # 3. OCR VIA DEEPDOCTECTION\n",
    "            result = client.predict(\n",
    "                img=handle_file(\"processed_table_region.png\"),\n",
    "                pdf=None,\n",
    "                max_datapoints=1,\n",
    "                api_name=\"/analyze_image\",\n",
    "            )\n",
    "\n",
    "            # 4. PARSE TABLE\n",
    "            if isinstance(result, tuple) and len(result) >= 4:\n",
    "                html_table = result[3]\n",
    "                soup = BeautifulSoup(html_table, \"html.parser\")\n",
    "                table = soup.find(\"table\")\n",
    "                df = pd.read_html(str(table))[0]\n",
    "\n",
    "                if df.columns.isnull().any():\n",
    "                    df.columns = [f\"col_{i}\" if pd.isna(c) else str(c).strip() for i, c in enumerate(df.columns)]\n",
    "                else:\n",
    "                    df.columns = [str(c).strip() for c in df.columns]\n",
    "\n",
    "                flat_values = df.to_numpy().ravel()\n",
    "                col_labels = []\n",
    "                for i in range(len(df)):\n",
    "                    for col in df.columns:\n",
    "                        col_labels.append(f\"{col}_{i+1}\")\n",
    "\n",
    "                if len(col_labels) != len(flat_values):\n",
    "                    min_len = min(len(col_labels), len(flat_values))\n",
    "                    col_labels = col_labels[:min_len]\n",
    "                    flat_values = flat_values[:min_len]\n",
    "\n",
    "                row_data = [datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")] + list(flat_values)\n",
    "                col_names = [\"timestamp\"] + col_labels\n",
    "\n",
    "                single_row = pd.DataFrame([row_data], columns=col_names)\n",
    "\n",
    "                header_needed = not os.path.exists(CSV_FILE)\n",
    "                single_row.to_csv(CSV_FILE, index=False, mode=\"a\", header=header_needed)\n",
    "\n",
    "                print(f\"[{iteration:3d}] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "\n",
    "            else:\n",
    "                print(f\"[{iteration}]  No table detected\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[{iteration}]  Error: {e}\")\n",
    "            continue\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        sleep_time = max(0, INTERVAL_SECONDS - elapsed)\n",
    "        if sleep_time > 0:\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n Stopped by user ‚Äî starting CSV cleanup...\")\n",
    "\n",
    "# ===============================\n",
    "# CLEANUP AND REFORMAT STORED CSV\n",
    "# ===============================\n",
    "if os.path.exists(CSV_FILE):\n",
    "    df = pd.read_csv(CSV_FILE)\n",
    "\n",
    "    BID_HEADERS = ['exch_b', 'size_b', 'bid', 'ask', 'size_a', 'exch_a']\n",
    "\n",
    "    # Keep timestamp + data columns (skip first 6 junk header columns)\n",
    "    data_columns = df.columns[6:]\n",
    "    df_data = df[['timestamp'] + list(data_columns)].copy()\n",
    "\n",
    "    # Drop first junk column and shift left by one\n",
    "    first_junk_col = data_columns[0]\n",
    "    df_data = df_data.drop(columns=[first_junk_col])\n",
    "\n",
    "    # Create proper alternating column names\n",
    "    col_names = [\"timestamp\"]\n",
    "    level = 1\n",
    "    for i in range(len(df_data.columns) - 1):  # Adjust for dropped column\n",
    "        base_header = BID_HEADERS[i % 6]\n",
    "        col_names.append(f\"{base_header}_{level}\")\n",
    "        if (i + 1) % 6 == 0:\n",
    "            level += 1\n",
    "\n",
    "    df_data.columns = col_names\n",
    "    df_data.to_csv(CLEAN_CSV_FILE, index=False)\n",
    "\n",
    "    print(f\"‚úÖ Cleaned CSV saved ‚Üí {CLEAN_CSV_FILE}\")\n",
    "    print(\"First 20 columns:\")\n",
    "    print(list(df_data.columns[:20]))\n",
    "    print(\"\\nSample preview:\")\n",
    "    print(df_data[['timestamp', 'exch_b_1', 'size_b_1', 'bid_1', 'ask_1', 'size_a_1', 'exch_a_1']].head(2))\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No CSV file found; skipping cleanup.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "20313869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             timestamp exch_b_1  size_b_1   bid_1   ask_1  size_a_1 exch_a_1  \\\n",
      "0  2025-12-06 09:48:15     ARCX         1  454.61  454.64       160     XNMS   \n",
      "1  2025-12-06 09:48:25     ARCX         1  454.61  454.64       160     XNMS   \n",
      "2  2025-12-06 09:48:34     ARCX         1  454.61  454.64       160     XNMS   \n",
      "\n",
      "  exch_b_2  size_b_2   bid_2   ask_2  size_a_2 exch_a_2 exch_b_3  size_b_3  \\\n",
      "0     XNMS        80  454.68  464.66      3884     ARCX     ARCX         3   \n",
      "1     XNMS        80  454.68  464.66      3884     ARCX     ARCX         3   \n",
      "2     XNMS        80  454.68  464.66      3884     ARCX     ARCX         3   \n",
      "\n",
      "    bid_3   ask_3  size_a_3 exch_a_3 exch_b_4  size_b_4  bid_4  ask_4  \\\n",
      "0  454.55  454.69        80     EDGX     ARCX       700  454.5  454.7   \n",
      "1  454.55  454.69        80     EDGX     ARCX       700  454.5  454.7   \n",
      "2  454.55  454.69        80     EDGX     ARCX       700  454.5  454.7   \n",
      "\n",
      "   size_a_4 exch_a_4 exch_b_5  size_b_5   bid_5   ask_5 size_a_5 exch_a_5  \\\n",
      "0       135     ARCX     EDGX        80  454.45  454.75        I     ARCX   \n",
      "1       135     ARCX     EDGX        80  454.45  454.75        I     ARCX   \n",
      "2       135     ARCX     EDGX        80  454.45  454.75        I     ARCX   \n",
      "\n",
      "    exch_b_6 size_b_6          bid_6          ask_6 size_a_6   exch_a_6  \\\n",
      "0  ARCX ARCX  560 198  454.45 454.40  454.78 454.79      I 1  ARCX ARCX   \n",
      "1  ARCX ARCX  560 198  454.45 454.40  454.78 454.79      I 1  ARCX ARCX   \n",
      "2  ARCX ARCX  560 198  454.45 454.40  454.78 454.79      I 1  ARCX ARCX   \n",
      "\n",
      "  exch_b_7  size_b_7   bid_7  ask_7  size_a_7 exch_a_7 exch_b_8  size_b_8  \\\n",
      "0     ARCX        21  454.35  454.8         2     ARCX     ARCX         1   \n",
      "1     ARCX        21  454.35  454.8         2     ARCX     ARCX         1   \n",
      "2     ARCX        21  454.35  454.8         2     ARCX     ARCX         1   \n",
      "\n",
      "   bid_8   ask_8  size_a_8 exch_a_8 exch_b_9  size_b_9   bid_9   ask_9  \\\n",
      "0  454.3  454.81        40     BATY     ARCX        21  454.25  454.83   \n",
      "1  454.3  454.81        40     BATY     ARCX        21  454.25  454.83   \n",
      "2  454.3  454.81        40     BATY     ARCX        21  454.25  454.83   \n",
      "\n",
      "   size_a_9 exch_a_9 exch_b_10  size_b_10  bid_10  ask_10  size_a_10  \\\n",
      "0         6     ARCX      ARCX         10  454.24  454.84          4   \n",
      "1         6     ARCX      ARCX         10  454.24  454.84          4   \n",
      "2         6     ARCX      ARCX         10  454.24  454.84          4   \n",
      "\n",
      "  exch_a_10  exch_b_11  size_b_11  bid_11  ask_11  size_a_11  exch_a_11  \n",
      "0      ARCX        NaN        NaN     NaN     NaN        NaN        NaN  \n",
      "1      ARCX        NaN        NaN     NaN     NaN        NaN        NaN  \n",
      "2      ARCX        NaN        NaN     NaN     NaN        NaN        NaN  \n",
      "\n",
      "üíæ Full dataset also saved to full_orderbook_view.csv (3 rows, 67 columns)\n"
     ]
    }
   ],
   "source": [
    "# Show ALL rows of df_data (temporarily override pandas display limits)\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.width', None):\n",
    "    print(df_data)\n",
    "\n",
    "# Or save to file for easier viewing\n",
    "df_data.to_csv(\"full_orderbook_view.csv\", index=False)\n",
    "print(f\"\\nüíæ Full dataset also saved to full_orderbook_view.csv ({len(df_data)} rows, {len(df_data.columns)} columns)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
